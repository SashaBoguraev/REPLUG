{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The first steps of our code is to install the necessary dependencies for our code, mount our drive, and set our device to GPU if available, else CPU."
      ],
      "metadata": {
        "id": "O0WJmpGhZFgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep0CY7DHh2Ii"
      },
      "outputs": [],
      "source": [
        "%pip install faiss-gpu\n",
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1YekFR7ediGD"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed, GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer, AutoModel, GPT2LMHeadModel\n",
        "from datasets import load_dataset\n",
        "import faiss, torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8EM74N0DbeB",
        "outputId": "faebd2d2-d4d6-42c2-d7f8-f0532629a444"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WyblLlpyRY9J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkRJusj5fxCf"
      },
      "source": [
        "# GPT2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now load in our GPT model and tokenizer. We further define two fucntions below:\n",
        "\n",
        "1. `get_logits`: which given a model, a tokenizer, and a set of text, will generate the models logits for next token prediction.\n",
        "2. `get_probs`: given a set of logits, it will return the corresponding probability distribution across the logits by taking the softmax."
      ],
      "metadata": {
        "id": "UyPk-1vAZZRs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eEf42IrdkZQ"
      },
      "outputs": [],
      "source": [
        "LM = GPT2LMHeadModel.from_pretrained(\"gpt2\").eval()\n",
        "LM_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side = \"left\")\n",
        "if LM_tokenizer.pad_token is None:\n",
        "  LM_tokenizer.pad_token = LM_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jB9FiAWqTV5f"
      },
      "outputs": [],
      "source": [
        "# Returns the logits for a given model\n",
        "def get_logits(text):\n",
        "  input_ids = LM_tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "  gen_tokens = LM.generate(\n",
        "      input_ids,\n",
        "      do_sample=True,\n",
        "      temperature=0.9,\n",
        "      max_length=100,\n",
        "      return_dict_in_generate=True,\n",
        "      output_logits=True,\n",
        "      pad_token_id=LM_tokenizer.pad_token_id\n",
        "  ).to_tuple()\n",
        "\n",
        "  return gen_tokens[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vlRacBH4PogY"
      },
      "outputs": [],
      "source": [
        "# Returns the probability of every vocab word at each of the sequence positions\n",
        "def get_probs(logits_tuple):\n",
        "  out_pad = logits_tuple[0]\n",
        "  out_pad_probs = out_pad.softmax(dim = -1)\n",
        "  return out_pad_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju__NnNpf0_3"
      },
      "source": [
        "# Contriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now load in our retriever model and tokenizer. As suggested in REPLUG, we use Facebook's Contriever model. This is a dense information retrieval model, and returns embeddings for given textual inputs"
      ],
      "metadata": {
        "id": "PLUOr_QwZ8UR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyKY1TjHf3fS"
      },
      "outputs": [],
      "source": [
        "retriever_tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
        "retriever = AutoModel.from_pretrained('facebook/contriever').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.save_pretrained(\"/content/drive/MyDrive/retriever\")"
      ],
      "metadata": {
        "id": "p6mE90NpDmy8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9j-LBTxi3xl"
      },
      "source": [
        "# Data (C4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also load in our dataset. While the original REPLUG paper uses The Pile dataset, this has since been taken down due to copyright issues. As such, we utilized AllenAI's C4 dataset. Furthermore, the original paper uses a datastore of 36M documents of 128 tokens, and a train dataset of 800K sequences of length 256 tokens. In comparison, we use 1000 sequences for our datastore and 500 sequences for training. This was due to memory constraints, we could not have larger datasets."
      ],
      "metadata": {
        "id": "ebKmiTbkabrd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl-vV9RUjdPf"
      },
      "outputs": [],
      "source": [
        "# Generate document dataset. In paper, 36M sequences of length 128 tokens.\n",
        "en = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.0000[1]-of-01024.json.gz\")[\"train\"].select(range(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we define two functions:\n",
        "1. `proc_sample`: Takes in a sample text, and retruns the retriever embeddings for that given piece of text.\n",
        "2. `make_ds`: Takes in a dataset, and makes a FAISS datastore."
      ],
      "metadata": {
        "id": "hBErVpKfcgcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_token_length = 128\n",
        "def proc_sample(sample):\n",
        "    out = retriever_tokenizer(sample[\"text\"], return_tensors=\"pt\", truncation=True, max_length=max_token_length, padding=\"max_length\")\n",
        "    input_ids = out.input_ids.to(device)\n",
        "    attention_mask = out.attention_mask.to(device)\n",
        "    a = (retriever(input_ids, attention_mask=attention_mask).last_hidden_state)\n",
        "    a[attention_mask==0] = 0.\n",
        "\n",
        "    return {'embeddings':a.mean(dim=-2)}\n",
        "\n",
        "def make_ds(en):\n",
        "  data_with_embeddings = en.map(proc_sample, batched=True, batch_size=75)\n",
        "  data_with_embeddings.set_format(\"pt\", columns=[\"embeddings\"], output_all_columns=True)\n",
        "  data_with_embeddings.add_faiss_index(column='embeddings', device=0)\n",
        "  return data_with_embeddings\n",
        "\n",
        "data_with_embeddings = make_ds(en)"
      ],
      "metadata": {
        "id": "UvkIb3WIYbgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below function, `get_top_responses`, takes in a set of queries, a datastore, a model, that model's tokenizer, and optionally a max token length and k value. With this, it returns the top-k retrieved pieces of context from the datastore, along with the similarity scores associated."
      ],
      "metadata": {
        "id": "y3MTz8pfcw0q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tR35sExck3h5"
      },
      "outputs": [],
      "source": [
        "# Get query embedding\n",
        "def get_top_responses(queries, datastore, model, tokenizer, max_token_length=128, k=20):\n",
        "  question_toks = tokenizer(queries, return_tensors=\"pt\", truncation=True, max_length=max_token_length, padding=\"max_length\")\n",
        "  input_ids = question_toks.input_ids.to(device)\n",
        "  attention_mask = question_toks.attention_mask.to(device)\n",
        "  a = (model(input_ids, attention_mask=attention_mask).last_hidden_state)\n",
        "  a[attention_mask==0] = 0.\n",
        "  a = a.mean(dim=-2).detach().cpu().numpy()\n",
        "  scores, retrieved_examples = datastore.get_nearest_examples_batch('embeddings', a, k=k)\n",
        "  return retrieved_examples, scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we load in our training data."
      ],
      "metadata": {
        "id": "ovu6a5i9dC9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYm8spCwUbgK"
      },
      "outputs": [],
      "source": [
        "train_test = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.0010[1]-of-01024.json.gz\")[\"train\"]\n",
        "train_seqs = train_test.select(range(5000))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_seqs = train_test.select(range(50000, 55000))"
      ],
      "metadata": {
        "id": "3xL3lVEycLCY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8qspLKuTd3f"
      },
      "source": [
        "# Putting Together"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define three functions that link together much of what we have defined above:\n",
        "\n",
        "1. `prob_gen`: Given a piece of context, a piece of text, a model and its tokenizer, returns the probabilities for LM's next token generation. Input to LM is context concatenated with the text itself.\n",
        "2. `get_LLM_outputs`: Given a query, datastore and k value, will return the ensemble probabilities across all top-k contexts appended to the query.\n",
        "3. `generate_text`: Given probabilities and a model tokenizer, returns the next token for generation."
      ],
      "metadata": {
        "id": "gvF9ftEJdF8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prob_gen(context, text):\n",
        "  out = LM_tokenizer(context, truncation=True, max_length=128, return_tensors='pt').input_ids.to(device)\n",
        "  t = LM_tokenizer(text, truncation=True, max_length=256, return_tensors=\"pt\").input_ids.to(device)\n",
        "  tot = torch.cat((out, t), dim=-1)\n",
        "  with torch.no_grad():\n",
        "    outputs = LM(tot)\n",
        "    logits = outputs.logits[0, len(out) + 1:]\n",
        "  log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "  return torch.exp(-log_probs.gather(-1, t).mean()).item()"
      ],
      "metadata": {
        "id": "5SuFbAmy6Z2d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JNJtnJHLmY5k"
      },
      "outputs": [],
      "source": [
        "# Set K to be the amount of repsonses to include\n",
        "def get_LLM_outputs(query, ds, k = 1):\n",
        "  top_responses, scores = get_top_responses(query, ds, k = k)\n",
        "\n",
        "  concat = lambda context: \"Context: \" + context + \"Query: \" + query\n",
        "\n",
        "  llm_logits = get_logits(concat(top_responses[0]))\n",
        "  llm_probs = get_probs(llm_logits)\n",
        "\n",
        "  for response in top_responses[1:]:\n",
        "    llm_logit = get_logits(concat(response))\n",
        "    llm_prob = get_probs(llm_logit)\n",
        "    llm_probs += llm_prob\n",
        "\n",
        "  llm_probs /= k\n",
        "  return llm_probs, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OuqJfCIvSnAt"
      },
      "outputs": [],
      "source": [
        "def generate_text(probs):\n",
        "  text_indices = torch.argmax(probs, dim = 1).item()\n",
        "  out_text = LM_tokenizer.convert_ids_to_tokens(text_indices)\n",
        "  if out_text[0] == 'Ġ':\n",
        "    out_text = out_text[1:]\n",
        "  return out_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning"
      ],
      "metadata": {
        "id": "BiX9GXP6V4dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we provide the code for fine-tuning. Sadly, we were unable to get past the memory constraints of Colab (without buying Colab pro), but through very small-scale tests, we believe our code to be correct. Individuals with enough memory on their Colab should be able to run this example with no issues.\n",
        "\n",
        "We define the following functions to do so:\n",
        "\n",
        "1. `loss`: Given a score computed in our `compute_score` function and LM generation probabilites, returns the loss as defined in the original paper.\n",
        "2. `compute_score`: Given a query and doc, will return the \"score\" denoting their similarity. In the case of our paper, this is just the cosine similarity of the embeddings.\n",
        "3. `train`: This possesses the main training loop of our fine-tuning. Our function takes in our model and tokenizer, as well as retriever and tokenizer and fine-tunes our retriever.\n",
        "\n",
        "\n",
        "*Note: We explicitly hardcode in the hyperparameters specified in the paper. If you would like to change these, please do so in the `loss` and `train` functions.*"
      ],
      "metadata": {
        "id": "CBaQwdlofD-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(scores, probs, gamma=0.8, beta=0.8):\n",
        "    num_contexts = len(probs)\n",
        "    P = torch.nn.functional.softmax(scores/gamma, dim=-1)  # retrieval likelihood\n",
        "    Q = torch.nn.functional.softmax(probs/beta, dim=-1)  # LM likelihood\n",
        "    return torch.mean(Q * (torch.log(Q) - torch.log(P)))"
      ],
      "metadata": {
        "id": "KJm-KHDlsSyV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_score(query, doc):\n",
        "  out = retriever(retriever_tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=256).input_ids.to(device)).last_hidden_state[0].mean(dim=-2)\n",
        "  doc_out = retriever(retriever_tokenizer(doc, return_tensors=\"pt\", truncation=True, max_length=128).input_ids.to(device)).last_hidden_state[0].mean(dim=-2)\n",
        "  return torch.nn.functional.cosine_similarity(out,doc_out, dim=-1)"
      ],
      "metadata": {
        "id": "CgOhS-Xn-m5J"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  retriever.train()\n",
        "  num_epochs = 20\n",
        "  recomp_epoch = 3000\n",
        "  batch_size = 64\n",
        "  optimizer = torch.optim.SGD(retriever.parameters(), lr = 2e-5)\n",
        "  warmup_ratio = 0.1\n",
        "  scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 1./(warmup_ratio*num_epochs), 1.0)\n",
        "  # do warmup ratio\n",
        "  train_size = len(train_seqs)\n",
        "  losses = []\n",
        "  for epoch in range(num_epochs):\n",
        "    for i in range(train_size//batch_size - 1):\n",
        "      i = np.random.randint(0, train_size//batch_size)\n",
        "      batch_size = len(range(i, max(i+batch_size, train_size)))\n",
        "      batch = train_seqs.select(range(i, max(i+batch_size, train_size)))[\"text\"]\n",
        "      optimizer.zero_grad()\n",
        "      batch_loss = torch.tensor(0.).to(device)\n",
        "      for j in range(batch_size):\n",
        "        examples, scores = get_top_responses(batch[j], data_with_embeddings)\n",
        "        examples = examples[0]\n",
        "        txt = batch[j]\n",
        "        scores = torch.zeros(20)\n",
        "        llm_probs = torch.zeros_like(scores)\n",
        "        for j, doc in enumerate(examples[\"text\"]):\n",
        "          document = doc\n",
        "          context = f\"Context: {document} \"\n",
        "          llm_probs[j] = prob_gen(context, \"<|endoftext|> \"+txt)\n",
        "          scores[j] = compute_score(txt, document)\n",
        "        batch_loss += loss(scores, llm_probs)\n",
        "      batch_loss.backward()\n",
        "      scheduler.step()\n",
        "      losses.append(batch_loss.item())\n",
        "    if epoch % 10 == 0:\n",
        "      print(losses[-1])\n",
        "    if epoch % recomp_epoch == 0 and epoch:\n",
        "      retriever.save_pretrained(f\"/content/drive/MyDrive/retriever_{epoch}\")\n",
        "      ds = make_ds(ds)"
      ],
      "metadata": {
        "id": "fDt1vUPFTy4m"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LM.to(device)\n",
        "# train()"
      ],
      "metadata": {
        "id": "sdcEu2DUuWuk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "iLzPw-u5WP_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we provide the code to evaluate the performance of our models. We provide four functions to aid us:\n",
        "\n",
        "1. `eval_batch`: We take in context and text and return the LM next token generation logits for the context appended to our text.\n",
        "2. `eval_gpt`: We take text and return the LM next token generation logits for the given text.\n",
        "3. `bpb_gpt_only`: Takes in a set of sequences, and calculates the bits per byte for GPT.\n",
        "4. `bpb_replug`:  Takes in a set of sequences, and calculates the bits per byte for GPT + REPLUG (evaluating with whichever retriever is loaded at a given moment)."
      ],
      "metadata": {
        "id": "4KEpsu7ThNK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8pcZMFuyvMVL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_batch(context, text):\n",
        "  out = LM_tokenizer(context, truncation=True, max_length=128, return_tensors='pt').input_ids.to(device)\n",
        "  t = LM_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "  tot = torch.cat((out, t), dim=-1)\n",
        "  with torch.no_grad():\n",
        "    outputs = LM(tot)\n",
        "    logits = outputs.logits[0, len(out[0] + 2):]\n",
        "  return logits"
      ],
      "metadata": {
        "id": "EhvOOrNlI0tB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_gpt(text):\n",
        "  t = LM_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "  with torch.no_grad():\n",
        "    outputs = LM(t, labels=t).loss\n",
        "  return outputs.item(), t.shape[-1]"
      ],
      "metadata": {
        "id": "nO0oO65DgkVt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bpb_replug(seqs, ds = None):\n",
        "  batch_size = 16\n",
        "  with torch.no_grad():\n",
        "    a = torch.tensor(0., device=device)\n",
        "    cnt=0\n",
        "    num_toks = 0\n",
        "    num_utf_bytes = 0\n",
        "    for i in tqdm(range(0, 5000, batch_size)):\n",
        "      if i+batch_size > 500:\n",
        "        break\n",
        "      inp = seqs.select(range(i,i+batch_size))\n",
        "      examples, scores = get_top_responses(inp[\"text\"], data_with_embeddings, retriever, retriever_tokenizer)\n",
        "      scores = torch.nn.functional.softmax(torch.tensor(scores, device=device), dim=-1)\n",
        "      text = inp['text']\n",
        "      for j,ex in enumerate(examples):\n",
        "        txt = text[j]\n",
        "        txt = \" \".join(txt.split(\" \")[:256])\n",
        "        logits = eval_batch(ex[\"text\"][0]+ \" \", \"<|endoftext|>  \"+txt) *scores[j][0]\n",
        "        input_toks = LM_tokenizer(\"<|endoftext|>  \"+txt, return_tensors=\"pt\").input_ids.to(device)\n",
        "        for k, doc in enumerate(ex[\"text\"][1:]):\n",
        "          logits += eval_batch(doc+ \" \", \"<|endoftext|>  \"+txt) *scores[j][k]\n",
        "        shift_toks = input_toks[:, 1:].squeeze(0)\n",
        "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)[:-1]\n",
        "        a+= torch.nn.functional.nll_loss(log_probs, shift_toks, reduction=\"mean\").item()\n",
        "        num_toks += shift_toks.shape[-1]\n",
        "        num_utf_bytes += len(txt.encode('utf-8'))\n",
        "        cnt+= 1\n",
        "    return num_toks/num_utf_bytes * a * np.log2(np.e)/cnt, num_toks/num_utf_bytes\n"
      ],
      "metadata": {
        "id": "5RDDFPoGBy25"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bpb_only_gpt(seqs, ds = None):\n",
        "  batch_size = 16\n",
        "  with torch.no_grad():\n",
        "    a = torch.tensor(0., device=device)\n",
        "    cnt=0\n",
        "    num_toks = num_utf_bytes = 0\n",
        "    for i in tqdm(range(0, 5000, batch_size)):\n",
        "      if i+batch_size > len(seqs): break\n",
        "      inp = seqs.select(range(i,i+batch_size))\n",
        "      text = inp['text']\n",
        "      for j in range(batch_size):\n",
        "        txt = text[j]\n",
        "        txt = \" \".join(txt.split(\" \")[:256])\n",
        "        l, s = eval_gpt(txt)\n",
        "        a+=l\n",
        "        num_toks += s\n",
        "        num_utf_bytes += len(txt.encode('utf-8'))\n",
        "        cnt+=1\n",
        "\n",
        "    return num_toks/num_utf_bytes * a * np.log2(np.e)/cnt, num_toks/num_utf_bytes"
      ],
      "metadata": {
        "id": "vIDPlxv21GR0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we actually evaluate our model's performances."
      ],
      "metadata": {
        "id": "BtaAxYZih8w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LM.to(device)\n",
        "\n",
        "print(\"Just GPT\")\n",
        "final_bpb_gpt = bpb_only_gpt(test_seqs)\n",
        "print(\"Final BPB: \", final_bpb_gpt[0].item())\n",
        "\n",
        "print(\"\\nRetriever\")\n",
        "retriever = AutoModel.from_pretrained(\"/content/drive/MyDrive/retriever\").to(device) # REPLACE THIS PATH WITH THE RETRIEVER YOU WANT TO EVALUATE\n",
        "final_bpb_replug = bpb_replug(test_seqs)\n",
        "print(\"\\nFinal BPB: \", final_bpb_replug[0].item())"
      ],
      "metadata": {
        "id": "vx99xeDJcyk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d13fefa7-16df-459b-fc48-ed365e77a2b1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just GPT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 312/313 [02:16<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final BPB:  1.171903371810913\n",
            "Retriever\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 31/313 [05:02<45:51,  9.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final BPB:  1.1895300149917603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}